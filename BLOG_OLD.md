# Building a Production-Ready CI/CD Pipeline on AWS ECS Fargate: A Complete Guide

## Why I Built This (And Why You Should Too)

When I started looking for DevOps jobs, I quickly realized something: everyone wants "hands-on experience," but how do you get that without a job? I decided to build a real-world project that would actually impress hiring managers.

This isn't another "hello world" deployment. This is a full-stack ticketing application with automated deployments, container orchestration, and all the buzzwords recruiters love to see on a CV. More importantly, it actually works.

By the end of this tutorial, you'll have:
- A full-stack application running on AWS ECS Fargate
- Automated CI/CD pipeline that deploys on every git push
- Docker containers in production
- Real DevOps experience you can talk about in interviews

**Time needed:** 3-4 hours  
**Cost:** Free tier eligible (or ~$10-15/month if you exceed limits)  
**Skill level:** Beginner-friendly (I'll explain everything)

---

## What We're Building

![Architecture Diagram]
**[SCREENSHOT 1: Place your architecture diagram here - the one showing GitHub ‚Üí CodePipeline ‚Üí CodeBuild ‚Üí ECR ‚Üí ECS Fargate]**

We're deploying **CloudCare** - a ticketing system I built with:
- **Backend:** Node.js + TypeScript + Express API
- **Frontend:** React + TypeScript + Vite
- **Database:** PostgreSQL on AWS RDS

The pipeline automatically:
1. Detects code changes in GitHub
2. Builds Docker images
3. Pushes to Amazon ECR
4. Deploys to ECS Fargate
5. Zero downtime (blue/green deployment)

---

## Prerequisites

Before we start, you'll need:

**Required:**
- [ ] AWS account (free tier works)
- [ ] GitHub account
- [ ] Basic terminal/command line knowledge
- [ ] Text editor (VS Code recommended)

**Helpful but not required:**
- Understanding of Docker concepts
- Basic knowledge of Node.js/React
- Familiarity with Git

**Don't worry if you're new to AWS or Docker** - I'll explain everything as we go.

---

## Part 1: Setting Up the Application

### Step 1: Clone the Repository

First, let's get the code. I'm using a separate repo for this CI/CD demo:

```bash
git clone https://github.com/GitHer-Muna/aws-ecs-fargate-cicd-pipeline.git
cd aws-ecs-fargate-cicd-pipeline
```

Or if you're starting from scratch with your own application, create a new repo:

```bash
mkdir aws-ecs-fargate-cicd-pipeline
cd aws-ecs-fargate-cicd-pipeline
git init
```

**[SCREENSHOT 2: Terminal showing the clone command and successful output]**

### Step 2: Understanding the Project Structure

Here's what we're working with:

```
aws-ecs-fargate-cicd-pipeline/
‚îú‚îÄ‚îÄ src/                    # Backend Node.js code
‚îú‚îÄ‚îÄ frontend/               # React frontend
‚îú‚îÄ‚îÄ prisma/                 # Database schema
‚îú‚îÄ‚îÄ Dockerfile.backend      # Backend container config (we'll create)
‚îú‚îÄ‚îÄ Dockerfile.frontend     # Frontend container config (we'll create)
‚îú‚îÄ‚îÄ buildspec.yml          # CodeBuild instructions (we'll create)
‚îú‚îÄ‚îÄ appspec.yml            # CodeDeploy config (we'll create)
‚îî‚îÄ‚îÄ task-definition-*.json # ECS configs (we'll create)
```

Don't panic if you don't have all these files yet - we're creating them together.

---

## Part 2: Dockerizing the Application

### What is Docker? (Quick Explanation)

Think of Docker as a way to package your application with everything it needs to run - code, dependencies, runtime - into a single container. It's like shipping your app in a standardized box that works anywhere.

### Step 3: Create Backend Dockerfile

Create a file called `Dockerfile.backend` in your project root:

```dockerfile
# Multi-stage build for Backend
FROM node:18-alpine AS builder

WORKDIR /app

# Copy package files
COPY package*.json ./
COPY prisma ./prisma/

# Install dependencies
RUN npm ci --only=production && \
    npm cache clean --force

# Copy source code
COPY . .

# Generate Prisma Client
RUN npx prisma generate

# Build TypeScript
RUN npm run build

# Production stage
FROM node:18-alpine

RUN apk add --no-cache dumb-init

# Create non-root user
RUN addgroup -g 1001 -S nodejs && \
    adduser -S nodejs -u 1001

WORKDIR /app

# Copy built application
COPY --from=builder --chown=nodejs:nodejs /app/dist ./dist
COPY --from=builder --chown=nodejs:nodejs /app/node_modules ./node_modules
COPY --from=builder --chown=nodejs:nodejs /app/prisma ./prisma
COPY --from=builder --chown=nodejs:nodejs /app/package*.json ./

USER nodejs

EXPOSE 3000

HEALTHCHECK --interval=30s --timeout=3s --start-period=40s \
    CMD node -e "require('http').get('http://localhost:3000/api/v1/health', (r) => {process.exit(r.statusCode === 200 ? 0 : 1)})"

CMD ["dumb-init", "node", "dist/index.js"]
```

**[SCREENSHOT 3: VS Code showing the Dockerfile.backend file]**

**What's happening here?**
- **Multi-stage build:** First stage builds the app, second stage runs it (smaller final image)
- **npm ci:** Cleaner install than npm install
- **Non-root user:** Security best practice
- **Health check:** AWS uses this to know if your container is healthy

### Step 4: Create Frontend Dockerfile

Create `Dockerfile.frontend`:

```dockerfile
# Multi-stage build for Frontend
FROM node:18-alpine AS builder

WORKDIR /app

# Copy frontend package files
COPY frontend/package*.json ./

# Install dependencies
RUN npm ci && npm cache clean --force

# Copy frontend source
COPY frontend/ ./

# Build the React app
RUN npm run build

# Production with Nginx
FROM nginx:alpine

# Copy nginx config
COPY frontend/nginx.conf /etc/nginx/conf.d/default.conf

# Copy built files
COPY --from=builder /app/dist /usr/share/nginx/html

EXPOSE 80

HEALTHCHECK --interval=30s --timeout=3s \
    CMD wget --quiet --tries=1 --spider http://localhost/health || exit 1

CMD ["nginx", "-g", "daemon off;"]
```

**Why Nginx?**  
React apps are just static files after building. Nginx is a lightweight web server perfect for serving them.

### Step 5: Create Nginx Configuration

Create `frontend/nginx.conf`:

```nginx
server {
    listen 80;
    server_name _;
    
    root /usr/share/nginx/html;
    index index.html;
    
    # Gzip compression
    gzip on;
    gzip_vary on;
    gzip_min_length 1024;
    gzip_types text/plain text/css text/xml text/javascript application/javascript;
    
    # Health check endpoint
    location /health {
        access_log off;
        return 200 "healthy\n";
        add_header Content-Type text/plain;
    }
    
    # React Router support
    location / {
        try_files $uri $uri/ /index.html;
    }
}
```

**[SCREENSHOT 4: The nginx.conf file in VS Code]**

### Step 6: Create .dockerignore

Create `.dockerignore` to exclude unnecessary files:

```
node_modules/
frontend/node_modules/
dist/
build/
.env
.git/
*.md
logs/
coverage/
```

This makes your Docker builds faster and images smaller.

---

## Part 3: Testing Docker Builds Locally

Before pushing to AWS, let's make sure everything works.

### Step 7: Build the Backend Image

```bash
docker build -f Dockerfile.backend -t cloudcare-backend:test .
```

**[SCREENSHOT 5: Terminal showing Docker build in progress]**

This will take a few minutes. You'll see it:
1. Downloading Node.js image
2. Installing dependencies
3. Building TypeScript
4. Creating the final image

If it succeeds, you'll see:
```
Successfully built abc123def456
Successfully tagged cloudcare-backend:test
```

**[SCREENSHOT 6: Successful build output]**

**Troubleshooting common errors:**

**Error: "Cannot find module"**  
‚Üí Make sure your package.json and package-lock.json are in the root directory

**Error: "ENOENT: no such file or directory"**  
‚Üí Check your file paths in the Dockerfile COPY commands

**Error: "npm ERR! peer dep missing"**  
‚Üí Try `npm install` locally first to generate a proper package-lock.json

### Step 8: Build the Frontend Image

```bash
docker build -f Dockerfile.frontend -t cloudcare-frontend:test .
```

Same process - this should complete faster since the frontend is smaller.

**[SCREENSHOT 7: Frontend build completion]**

### Step 9: Verify Images

Check that both images were created:

```bash
docker images | grep cloudcare
```

You should see:
```
cloudcare-backend     test    abc123    2 minutes ago    150MB
cloudcare-frontend    test    def456    1 minute ago     25MB
```

**[SCREENSHOT 8: Docker images list]**

Great! Docker part is done. Now let's set up AWS.

---

## Part 4: AWS Infrastructure Setup

This is where it gets exciting. We're building production infrastructure!

### Step 10: Create AWS Account and Setup

If you don't have an AWS account:
1. Go to aws.amazon.com
2. Click "Create an AWS Account"
3. Follow the signup process (you'll need a credit card, but we're using free tier)

**[SCREENSHOT 9: AWS Console dashboard after login]**

### Step 11: Create ECR Repositories

ECR (Elastic Container Registry) is where we'll store our Docker images.

1. Open AWS Console
2. Search for "ECR" in the services search bar
3. Click "Get Started" or "Create repository"

**[SCREENSHOT 10: ECR service page]**

**Create Backend Repository:**
- Repository name: `cloudcare-backend`
- Leave other settings as default
- Click "Create repository"

**[SCREENSHOT 11: Creating ECR repository]**

**Repeat for Frontend:**
- Repository name: `cloudcare-frontend`
- Create repository

You should now have two repositories:

**[SCREENSHOT 12: Both ECR repositories listed]**

**Copy the repository URIs** - you'll need them later. They look like:
```
123456789012.dkr.ecr.us-east-1.amazonaws.com/cloudcare-backend
123456789012.dkr.ecr.us-east-1.amazonaws.com/cloudcare-frontend
```

### Step 12: Create RDS Database

We need a PostgreSQL database for the backend.

1. Search for "RDS" in AWS Console
2. Click "Create database"

**[SCREENSHOT 13: RDS creation page]**

**Configuration:**
- Engine: PostgreSQL
- Version: PostgreSQL 15 (or latest)
- Templates: **Free tier** (important!)
- DB instance identifier: `cloudcare-db`
- Master username: `postgres`
- Master password: Create a strong password (save it somewhere safe!)
- DB instance class: db.t3.micro (free tier)
- Storage: 20 GB (default)
- Public access: **No**
- VPC security group: Create new ‚Üí `cloudcare-db-sg`

**[SCREENSHOT 14: RDS configuration settings]**

Click "Create database" - this takes 5-10 minutes.

**[SCREENSHOT 15: RDS database creation in progress]**

While waiting, let's set up more infrastructure.

### Step 13: Create VPC and Security Groups

Your containers need to communicate with the database and internet.

**Create Security Group for Backend:**
1. Search for "VPC" in AWS Console
2. Click "Security Groups" in left sidebar
3. Click "Create security group"

**[SCREENSHOT 16: Security group creation]**

**Configuration:**
- Name: `cloudcare-backend-sg`
- Description: Backend ECS tasks
- VPC: (select your default VPC)

**Inbound rules:**
- Type: Custom TCP
- Port: 3000
- Source: Anywhere (0.0.0.0/0) - for testing; restrict this in production

**[SCREENSHOT 17: Backend security group inbound rules]**

**Create Security Group for Frontend:**
- Name: `cloudcare-frontend-sg`
- Port: 80
- Source: Anywhere (0.0.0.0/0)

**Update Database Security Group:**
Find the `cloudcare-db-sg` security group:
- Add inbound rule
- Type: PostgreSQL
- Port: 5432
- Source: `cloudcare-backend-sg` (select the backend security group)

**[SCREENSHOT 18: Database security group allowing backend access]**

This allows only the backend to connect to the database.

### Step 14: Create ECS Cluster

ECS (Elastic Container Service) will run our containers.

1. Search for "ECS"
2. Click "Clusters" ‚Üí "Create Cluster"

**[SCREENSHOT 19: ECS cluster creation]**

**Configuration:**
- Cluster name: `cloudcare-cluster`
- Infrastructure: AWS Fargate (serverless)
- Leave everything else as default

**[SCREENSHOT 20: Cluster configuration]**

Click "Create" - it creates instantly.

**[SCREENSHOT 21: Cluster created successfully]**

### Step 15: Create Application Load Balancer

The load balancer distributes traffic to your containers.

1. Search for "EC2"
2. Left sidebar ‚Üí "Load Balancers"
3. Click "Create Load Balancer"
4. Choose "Application Load Balancer"

**[SCREENSHOT 22: ALB creation page]**

**Configuration:**
- Name: `cloudcare-alb`
- Scheme: internet-facing
- IP address type: IPv4
- VPC: (default VPC)
- Mappings: Select **at least 2 availability zones**

**[SCREENSHOT 23: ALB network configuration]**

**Security groups:**
- Create new: `cloudcare-alb-sg`
- Inbound: Port 80 from anywhere

**Listeners and routing:**
- Protocol: HTTP
- Port: 80
- Default action: Create target group

**[SCREENSHOT 24: ALB listener configuration]**

**Create Target Group for Backend:**
- Target type: IP
- Name: `cloudcare-backend-tg`
- Protocol: HTTP
- Port: 3000
- VPC: (default)
- Health check path: `/api/v1/health`

**[SCREENSHOT 25: Target group creation]**

Create the load balancer. This takes 2-3 minutes.

**[SCREENSHOT 26: ALB provisioning]**

**Repeat for Frontend Target Group:**
- Name: `cloudcare-frontend-tg`
- Port: 80
- Health check path: `/health`

### Step 16: Store Secrets in AWS Secrets Manager

Never hardcode passwords! Let's use Secrets Manager.

1. Search for "Secrets Manager"
2. Click "Store a new secret"

**[SCREENSHOT 27: Secrets Manager page]**

**Create Database URL Secret:**
- Secret type: Other type of secret
- Key/value pairs:
  - Key: `DATABASE_URL`
  - Value: `postgresql://postgres:YOUR_PASSWORD@cloudcare-db.ENDPOINT:5432/postgres`

Replace:
- `YOUR_PASSWORD` with your RDS password
- `ENDPOINT` with your RDS endpoint (find in RDS dashboard)

- Secret name: `cloudcare/database-url`

**[SCREENSHOT 28: Creating database secret]**

**Create JWT Secret:**
- Secret type: Other type of secret
- Key: `JWT_SECRET`
- Value: Generate a random 64-character string (use: `openssl rand -base64 64`)
- Secret name: `cloudcare/jwt-secret`

**Create JWT Refresh Secret:**
- Key: `JWT_REFRESH_SECRET`
- Value: Another random 64-character string
- Secret name: `cloudcare/jwt-refresh-secret`

**[SCREENSHOT 29: All three secrets created]**

---

## Part 5: Creating AWS Configuration Files

Now we create the files that tell AWS how to build and deploy.

### Step 17: Create buildspec.yml

This tells CodeBuild how to build your Docker images.

Create `buildspec.yml` in project root:

```yaml
version: 0.2

phases:
  pre_build:
    commands:
      - echo Logging in to Amazon ECR...
      - aws --version
      - aws ecr get-login-password --region $AWS_DEFAULT_REGION | docker login --username AWS --password-stdin $AWS_ACCOUNT_ID.dkr.ecr.$AWS_DEFAULT_REGION.amazonaws.com
      - REPOSITORY_URI_BACKEND=$AWS_ACCOUNT_ID.dkr.ecr.$AWS_DEFAULT_REGION.amazonaws.com/$IMAGE_REPO_NAME_BACKEND
      - REPOSITORY_URI_FRONTEND=$AWS_ACCOUNT_ID.dkr.ecr.$AWS_DEFAULT_REGION.amazonaws.com/$IMAGE_REPO_NAME_FRONTEND
      - COMMIT_HASH=$(echo $CODEBUILD_RESOLVED_SOURCE_VERSION | cut -c 1-7)
      - IMAGE_TAG=${COMMIT_HASH:=latest}

  build:
    commands:
      - echo Build started on `date`
      - echo Building Backend Docker image...
      - docker build -f Dockerfile.backend -t $REPOSITORY_URI_BACKEND:latest -t $REPOSITORY_URI_BACKEND:$IMAGE_TAG .
      - echo Building Frontend Docker image...
      - docker build -f Dockerfile.frontend -t $REPOSITORY_URI_FRONTEND:latest -t $REPOSITORY_URI_FRONTEND:$IMAGE_TAG .

  post_build:
    commands:
      - echo Build completed on `date`
      - echo Pushing Docker images...
      - docker push $REPOSITORY_URI_BACKEND:latest
      - docker push $REPOSITORY_URI_BACKEND:$IMAGE_TAG
      - docker push $REPOSITORY_URI_FRONTEND:latest
      - docker push $REPOSITORY_URI_FRONTEND:$IMAGE_TAG
      - echo Writing image definitions file...
      - printf '[{"name":"backend","imageUri":"%s"},{"name":"frontend","imageUri":"%s"}]' $REPOSITORY_URI_BACKEND:$IMAGE_TAG $REPOSITORY_URI_FRONTEND:$IMAGE_TAG > imagedefinitions.json

artifacts:
  files:
    - imagedefinitions.json
    - appspec.yaml
```

**[SCREENSHOT 30: buildspec.yml in VS Code]**

### Step 18: Create Task Definitions

Create `task-definition-backend.json`:

```json
{
  "family": "cloudcare-backend",
  "networkMode": "awsvpc",
  "requiresCompatibilities": ["FARGATE"],
  "cpu": "512",
  "memory": "1024",
  "executionRoleArn": "arn:aws:iam::YOUR_ACCOUNT_ID:role/ecsTaskExecutionRole",
  "taskRoleArn": "arn:aws:iam::YOUR_ACCOUNT_ID:role/ecsTaskRole",
  "containerDefinitions": [
    {
      "name": "backend",
      "image": "YOUR_ACCOUNT_ID.dkr.ecr.us-east-1.amazonaws.com/cloudcare-backend:latest",
      "portMappings": [
        {
          "containerPort": 3000,
          "protocol": "tcp"
        }
      ],
      "essential": true,
      "environment": [
        {
          "name": "NODE_ENV",
          "value": "production"
        },
        {
          "name": "PORT",
          "value": "3000"
        }
      ],
      "secrets": [
        {
          "name": "DATABASE_URL",
          "valueFrom": "arn:aws:secretsmanager:us-east-1:YOUR_ACCOUNT_ID:secret:cloudcare/database-url"
        },
        {
          "name": "JWT_SECRET",
          "valueFrom": "arn:aws:secretsmanager:us-east-1:YOUR_ACCOUNT_ID:secret:cloudcare/jwt-secret"
        }
      ],
      "logConfiguration": {
        "logDriver": "awslogs",
        "options": {
          "awslogs-group": "/ecs/cloudcare-backend",
          "awslogs-region": "us-east-1",
          "awslogs-stream-prefix": "ecs"
        }
      },
      "healthCheck": {
        "command": ["CMD-SHELL", "node -e \"require('http').get('http://localhost:3000/api/v1/health', (r) => {process.exit(r.statusCode === 200 ? 0 : 1)})\""],
        "interval": 30,
        "timeout": 5,
        "retries": 3,
        "startPeriod": 60
      }
    }
  ]
}
```

**Replace:**
- `YOUR_ACCOUNT_ID` with your AWS account ID (12-digit number)
- Region if not using `us-east-1`

**[SCREENSHOT 31: Task definition file]**

Create `task-definition-frontend.json` (similar structure, port 80).

### Step 19: Create IAM Roles

We need two roles for ECS:

**Create ecsTaskExecutionRole:**
1. Go to IAM ‚Üí Roles ‚Üí Create role
2. Trusted entity: AWS service ‚Üí Elastic Container Service ‚Üí Elastic Container Service Task

**[SCREENSHOT 32: IAM role creation]**

3. Attach policies:
   - `AmazonECSTaskExecutionRolePolicy`
   - `SecretsManagerReadWrite` (for reading secrets)

**[SCREENSHOT 33: Attaching policies]**

4. Role name: `ecsTaskExecutionRole`

**Create ecsTaskRole:**
- Same process
- Attach: `AmazonECSTaskExecutionRolePolicy`
- Name: `ecsTaskRole`

---

## Part 6: Setting Up CI/CD Pipeline

Almost there! Now we connect everything together.

### Step 20: Create CodeBuild Project

1. Search for "CodeBuild"
2. Click "Create build project"

**[SCREENSHOT 34: CodeBuild project creation]**

**Configuration:**
- Project name: `cloudcare-build`
- Source provider: GitHub
- Repository: Connect your GitHub repo

**[SCREENSHOT 35: Connecting GitHub repository]**

- Environment:
  - Managed image
  - Operating system: Ubuntu
  - Runtime: Standard
  - Image: `aws/codebuild/standard:7.0`
  - Privileged: **Enable** (required for Docker)

**[SCREENSHOT 36: Build environment settings]**

- Service role: Create new
- Buildspec: Use a buildspec file (`buildspec.yml`)

**Environment variables:**
Add these:
- `AWS_ACCOUNT_ID` = your 12-digit account ID
- `AWS_DEFAULT_REGION` = your region
- `IMAGE_REPO_NAME_BACKEND` = `cloudcare-backend`
- `IMAGE_REPO_NAME_FRONTEND` = `cloudcare-frontend`

**[SCREENSHOT 37: Environment variables configuration]**

Create the build project.

### Step 21: Create ECS Services

Now we create the services that run our containers.

1. Go to ECS ‚Üí Clusters ‚Üí cloudcare-cluster
2. Click "Create" under Services

**[SCREENSHOT 38: ECS service creation]**

**Backend Service:**
- Launch type: Fargate
- Task definition: cloudcare-backend (you'll need to register it first)
- Service name: `cloudcare-backend-service`
- Number of tasks: 1
- VPC: default
- Subnets: Select your subnets
- Security group: `cloudcare-backend-sg`
- Load balancer: Application Load Balancer
- Load balancer name: cloudcare-alb
- Target group: cloudcare-backend-tg

**[SCREENSHOT 39: Backend service configuration]**

**Frontend Service:**
- Same steps but use frontend task definition and target group

**[SCREENSHOT 40: Both services running]**

### Step 22: Create CodePipeline

This is the final piece - it ties everything together!

1. Search for "CodePipeline"
2. Click "Create pipeline"

**[SCREENSHOT 41: CodePipeline creation]**

**Pipeline settings:**
- Pipeline name: `cloudcare-pipeline`
- Service role: Create new

**Source stage:**
- Source provider: GitHub (Version 2)
- Connection: Create new connection
- Repository: your-username/aws-ecs-fargate-cicd-pipeline
- Branch: main
- Output format: CodePipeline default

**[SCREENSHOT 42: Source stage configuration]**

**Build stage:**
- Build provider: AWS CodeBuild
- Project name: cloudcare-build

**[SCREENSHOT 43: Build stage configuration]**

**Deploy stage:**
- Deploy provider: Amazon ECS
- Cluster: cloudcare-cluster
- Service: cloudcare-backend-service

**[SCREENSHOT 44: Deploy stage configuration]**

Create the pipeline!

**[SCREENSHOT 45: Pipeline created and running first deployment]**

---

## Part 7: Testing the Pipeline

### Step 23: Push a Change

Let's test if everything works!

Make a small change to your code:

```bash
# Edit a file
echo "// Pipeline test" >> src/index.ts

# Commit and push
git add .
git commit -m "Test CI/CD pipeline"
git push origin main
```

**[SCREENSHOT 46: Git push command]**

### Step 24: Watch the Magic

Go to CodePipeline in AWS Console. You should see:

**[SCREENSHOT 47: Pipeline triggered automatically]**

Watch it progress through:
1. ‚úÖ Source (pulls from GitHub)
2. ‚è≥ Build (building Docker images)
3. ‚è≥ Deploy (deploying to ECS)

**[SCREENSHOT 48: Build stage in progress showing logs]**

**[SCREENSHOT 49: Deploy stage in progress]**

After 5-10 minutes:

**[SCREENSHOT 50: Pipeline execution succeeded - all green checkmarks]**

### Step 25: Access Your Application

Get your Load Balancer DNS:
1. Go to EC2 ‚Üí Load Balancers
2. Copy the DNS name

**[SCREENSHOT 51: Finding ALB DNS name]**

Open in browser:
- Backend: `http://your-alb-dns.amazonaws.com/api/v1/health`
- Frontend: `http://your-alb-dns.amazonaws.com`

**[SCREENSHOT 52: Application running successfully in browser]**

üéâ **IT WORKS!**

---

## Part 8: Monitoring and Troubleshooting

### Viewing Logs

CloudWatch has all your logs:

1. Go to CloudWatch ‚Üí Log groups
2. Find `/ecs/cloudcare-backend`

**[SCREENSHOT 53: CloudWatch log groups]**

3. Click to view logs from your containers

**[SCREENSHOT 54: Container logs in CloudWatch]**

### Common Issues and Fixes

**Issue: Build fails with "Cannot find module"**
- Check your package.json is committed
- Make sure dependencies are in dependencies, not devDependencies

**Issue: Task fails health check**
- Check your health endpoint works: `/api/v1/health`
- Verify security groups allow traffic on the right ports
- Look at container logs in CloudWatch

**Issue: Database connection failed**
- Check security group allows backend to connect to RDS
- Verify DATABASE_URL is correct in Secrets Manager
- Ensure RDS is in same VPC as ECS tasks

**Issue: Pipeline stuck**
- Check IAM roles have correct permissions
- Verify CodeBuild has Docker privileged mode enabled
- Check ECR repository names match environment variables

---

## Cost Optimization Tips

Running this costs money. Here's how to minimize costs:

**Free tier eligible:**
- 750 hours/month of t2.micro EC2 (covered by Fargate free tier)
- 50 GB ECR storage
- 1 million requests on ALB

**To save money:**
1. Stop ECS services when not in use
2. Delete RDS when done testing (take snapshot first)
3. Use t3.micro for RDS (cheapest option)
4. Set up billing alerts

**Set up billing alert:**
1. Go to CloudWatch ‚Üí Alarms
2. Create alarm for "EstimatedCharges"
3. Set threshold: $10

**[SCREENSHOT 55: Billing alarm configuration]**

---

## What I Learned (The Real Talk)

Building this taught me way more than any tutorial:

**Technical Skills:**
- Docker isn't that scary once you build a few images
- AWS has a million moving parts, but you only need to understand a few for most projects
- Security groups are basically fancy firewalls
- IAM roles are confusing at first but make sense once you think of them as "permission sets"

**What I'd Do Differently:**
- Start with just the backend, then add frontend (I tried both at once and got confused)
- Use CloudFormation or Terraform from the start (manual clicking gets old fast)
- Set up monitoring earlier (spent hours debugging without proper logs)

**What Surprised Me:**
- How fast Fargate is compared to EC2
- How expensive RDS can get (use Aurora Serverless for production)
- How easy it is to rack up costs if you forget to delete resources

---

## Next Steps: Make It Your Own

This is a great start, but here's how to make it even better for your portfolio:

**Level 2 - Intermediate:**
- [ ] Add SSL/HTTPS with ACM (AWS Certificate Manager)
- [ ] Set up custom domain with Route 53
- [ ] Implement blue/green deployments with CodeDeploy
- [ ] Add CloudFront CDN for frontend

**Level 3 - Advanced:**
- [ ] Convert to Infrastructure as Code (Terraform/CloudFormation)
- [ ] Add auto-scaling based on CPU/memory
- [ ] Implement canary deployments
- [ ] Set up Prometheus + Grafana monitoring
- [ ] Add AWS WAF for security

**Level 4 - Expert:**
- [ ] Multi-region deployment
- [ ] Disaster recovery setup
- [ ] Cost optimization with Savings Plans
- [ ] Implement GitOps with FluxCD

---

## The Resume Boost

Here's how I talk about this project in interviews:

**"I built a production-ready CI/CD pipeline on AWS ECS Fargate that automatically deploys a full-stack application from GitHub. The pipeline uses CodePipeline, CodeBuild, and CodeDeploy for zero-downtime deployments. I containerized both the Node.js backend and React frontend using Docker multi-stage builds, stored images in ECR, and deployed to Fargate with auto-scaling. I implemented infrastructure security with VPC isolation, security groups, and secrets management. The entire deployment is automated - every git push triggers a new production deployment."**

Boom. That's the kind of answer that gets you hired.

---

## Conclusion

You now have:
- ‚úÖ A real production deployment on AWS
- ‚úÖ Automated CI/CD pipeline
- ‚úÖ Docker experience
- ‚úÖ AWS services knowledge (ECS, ECR, RDS, CodePipeline, etc.)
- ‚úÖ Something impressive to put on your resume
- ‚úÖ Practical knowledge you can talk about in interviews

**Total time:** About 3-4 hours if you follow this guide  
**Total cost:** ~$10-15/month (or free if you stay in free tier limits)  
**Career value:** Priceless

---

## Resources

- [GitHub Repository](https://github.com/GitHer-Muna/aws-ecs-fargate-cicd-pipeline)
- [AWS ECS Documentation](https://docs.aws.amazon.com/ecs/)
- [Docker Documentation](https://docs.docker.com/)
- [My Other DevOps Projects](https://github.com/GitHer-Muna)

---

## Questions?

Drop them in the comments! I read and respond to every one.

If this helped you, please:
- ‚≠ê Star the GitHub repo
- üëè Clap for this article
- üîó Share with fellow DevOps learners

Good luck on your DevOps journey! üöÄ

---

**About Me**  
I'm an aspiring DevOps engineer documenting my learning journey. Follow me for more hands-on tutorials and real-world projects.

[LinkedIn] | [GitHub] | [Twitter]

---

*Last updated: December 2025*
